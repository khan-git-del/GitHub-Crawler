name: Crawl Repositories
on: [push]
jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
    - uses: actions/checkout@v3
    - uses: actions/setup-python@v4
      with: {python-version: '3.11'}
    - run: pip install aiohttp asyncpg
    - run: python main.py
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/github_crawler
    - name: Export and Upload
      run: |
        pg_dump -h localhost -U postgres -t repositories github_crawler > repos.csv
        aws s3 cp repos.csv s3://your-bucket/
      env:
        PGPASSWORD: postgres
